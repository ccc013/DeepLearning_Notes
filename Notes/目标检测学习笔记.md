# 目标检测学习笔记

## 1. 目标定位(object localization)

图像分类、目标定位以及检测的区别如下图所示，前两个是图片中只有 1 个对象的情况，而检测是图片有多个对象的情况。

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/%E7%9B%AE%E6%A0%87%E5%AE%9A%E4%BD%8D.png)

所以，目标定位实际上是在分类的基础上定位到对象的位置，即找到对象在哪里并且直到这个对象是属于哪一类。

在图像分类中，一般定义的标签 y 的维度和类别是一样的，即假如是有 3 个类别，那么标签 y 的维度也是 3 个，比如令 $y=[c_1, c_2, c_3]$ ，然后输出的时候就判断哪个类别的预测概率大，就将其作为该对象的预测类别。

而在目标定位中，增加了寻找对象的位置的工作，那么标签就需要有坐标信息，所以这里假设是 3 个类别，定义其标签为：
$$
y = \ \begin{bmatrix}p_c \\ b_x \\ b_y \\ b_h \\ b_w \\ c_1 \\ c_2 \\ c_3 \\\end{bmatrix}
$$
其中，$p_c$ 表示图片是否包含有对象，如果对象属于指定的3 个类别中的一个，那么$p_c=1$, 否则就是 $p_c=0$，然后接下来的 $b_x, b_y, b_h, b_w$ 表示的就是坐标，或者说就是边界框参数，一般来说就是左上角的坐标加上边界框的宽和高，然后最后 3 个就是代表类别了，有多少个类别，就有多少个参数，其数值表示的预测概率。

然后神经网络的损失函数，一般就是采用平方误差策略，假设类别 $y$ 和网络的输出 $\hat{y}$，那么损失函数就是这么计算了,根据上述的标签定义，是有 9 维的：
$$
L(\hat{y}, y) = (\hat{y_1}-y_1)^2+(\hat{y_2}-y_2)^2+\dots +(\hat{y_8}-y_8)^2
$$
当然了，这里是用平方误差简化了，实际应用中，通常做法是对边界框的坐标应用平方差或者类似方法，对 $p_c$ 应用逻辑回归函数，或者评分预测误差，而对类别标签应用对数损失函数。



## 2. 基于滑动窗口的目标检测算法

对于基于滑动窗口的目标检测算法，首先是创建一个标签训练集，也就是将图片进行剪切成多个图片样本，如下图所示，将左图进行剪切，得到中间的 5个样本，然后按照样本是否包含汽车，进行标注标签，然后将这个训练集输入 CNN 中，训练得到一个模型。

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/%E5%9F%BA%E4%BA%8E%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E7%9A%84%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B.png)

当训练好模型后，就可以进行测试，测试的例子如下所示，选择一个特定大小的窗口，然后从图片左上角开始滑动，每次将窗口内的图片送入模型中，判断该图片内是否有汽车，依次重复操作，直到滑动窗口滑过测试图片的每个角落。

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/%E5%9F%BA%E4%BA%8E%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E7%9A%84%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%202.png)

上述做法就是**滑动窗口目标检测**。以某个步幅滑动这些方框窗口遍历整张图片，对这些方形区域进行分类，判断是否包含目标对象。

**该算法的一个很明显的缺点，就是计算成本**。主要原因是跟滑动窗口的大小有关系，选择太小的，那么就会需要滑动很多次，也就是需要检测多个小窗口，提高了计算成本；而如果窗口过大，那么窗口数量会减少，但是会影响模型的性能。

### 滑动窗口的卷积实现

上述介绍的实现基于滑动窗口的目标检测的方法，效率是比较低，这里会介绍如何通过卷积实现滑动窗口，首先需要将 CNN 的全连接层转换为卷积层，也就是得到一个全卷积网络（FCN），如下图所示：

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E7%9A%84%E5%8D%B7%E7%A7%AF%E5%AE%9E%E7%8E%B0.png)

这里的输入图片例子是一个 $14\times 14\times 3$ 的图片，然后经过一个卷积核大小是 $5\times 5$ 的卷积层，输出是 $14\times 14\times 3$ ，接着是一个 Max pooling 层，参数是 $2\times 2$ ，输出就是 $5\times 5\times 16$ ，原本是接着两个 $400\times 400$ 的全连接层，现在改为用 $1\times 1\times 400$ 的两个卷积层。

接着是主要参考论文 **OverFeat** 来介绍如何通过卷积实现滑动窗口对象检测算法。

具体实现例子如下所示，第一行表示训练时候使用 $14\times 14\times 3$的图片，第二行表示测试时候使用的输入图片大小是 $16\times 16\times 3$。而使用这个图片，在经过卷积层的时候，这里步幅是 2，所以卷积核是移动了四次，得到了输出是 $12\times 12\times 16$，最终的输出也是 $2\times 2\times 4$。

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E7%9A%84%E5%8D%B7%E7%A7%AF%E5%AE%9E2.png)

可以看到，其实在这 4 次卷积操作中有很多计算是重复的，因为有很多区域都是重叠的，具体四次如下所示，不同颜色的框表示四次操作的范围，左边第一个图的红色，然后移动 2 格，是第二个图中绿色框的区域，接着是第三张图里橙色，也就是左下角，然后第四张图里右下角，其实中间区域都是重叠的，也就是四个角落是有所不同。

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E7%9A%84%E5%8D%B7%E7%A7%AF%E5%AE%9E%E7%8E%B0%203.png)

简单说，通过这个卷积操作，我们就可以不用将测试图片分割成 4 个子图片，分别输入网络中，执行前向操作，进行预测，直接整张图输入网络即可，卷积层就会帮我们完成这个操作，也就是一次前向操作即可，节省了 4 倍的时间。



**不过，这种方法虽然提高了算法的效率，但也有一个缺点，就是边界框的位置可能不够准确。**



### Bounding Box预测（Bounding box predictions）

接下来要介绍如何可以得到精确的边界框，这里介绍的就是著名的 **YOLO（You only look once） 算法**，目前也是目标检测里很常用的一种算法，以及有了更多的版本，从最初的 YOLO，到目前的 YOLOv5，持续进行改进和提升。

YOLO 算法的做法如下图所示，采用一个 $3\times 3$ 的网格，将输入图片分成了 9 个区域，然后检测每个区域内是否有目标对象，YOLO 算法会将检测到的对象，根据其中点位置，将其分配到中点所在的格子里，所以下图中编号 4 和 6 包含了汽车，但是编号 5 虽然同时有两辆车的一部分，但因为中心点不在，所以这个格子输出的结果是不包含有汽车。

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/YOLO%20%E7%AE%97%E6%B3%95.png )



采用这个算法，网络输出的结果就是 $3\times 3 \times 8$ , 这里表示 $3\times 3$ 的网格，每个网格的结果是一个 8 维的向量，也是之前定义好的，即 $p_c, b_x, b_y, b_w, b_h, c_1, c_2, c_3$ 。

**该算法的优点就是CNN 可以输、出精确的边界框**，在实践中可以采用更多的网格，比如 $19\times 19$，即便图片中包含多个对象，但如果网格数量越多，每个格子就越小，一个格子存在多个对象的概率就会很低。

YOLO 算法的另一个优点是它采用卷积实现，速度非常快，这也是它很受欢迎的原因。

### 交并比（Intersection over union）

交并比(IoU)表示两个边界框交集和并集之比。并集就是如下图中绿色区域部分，即同时包含两个边界框的区域；而交集就是两个边界框重叠部分，下图中橙色区域。所以交并比就是橙色区域面积除以绿色区域的面积。

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/%E4%BA%A4%E5%B9%B6%E6%AF%94.png)

一般来说，IoU 大于等于 0.5，就可以说检测正确，结果是可以接受的，这也是一般的约定。但IoU 越大，边界框就约精确了。

这也是衡量定位精确到的一种方式，IoU 是衡量了两个边界框重叠的相对大小。



## 3. 非极大值抑制

目前的检测算法还会存在一个问题，就是对同一个对象作出多次的检测，而非极大值抑制就可以确保算法只对每个对象检测一次。

非极大值抑制算法的执行过程如下图所示，这里是采用 $19\times 19$ 的网格，对每个网格先执行检测算法，得到的输出就是 $19\times 19 \times 8$。当然这里只是预测是否有汽车，那么其实可以暂时不需要分类部分，也就是每个网格输出一个 5 维向量，$p_c$ 以及边界框的四个坐标参数。

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/%E9%9D%9E%E6%9E%81%E5%A4%A7%E5%80%BC%E6%8A%91%E5%88%B6.png)



然后开始实现非极大值抑制算法：

1. 去掉所有预测概率低于阈值的边界框，比如设置阈值是 0.6，那么对于 $p_c \le 0.6$ 的边界框都被抛弃；
2. 在剩下的边界框里，将预测概率最高的边界框，将其输出作为预测结果；
3. 然后将还剩下的边界框里，和第一步被抛弃的边界框有高 IoU 的，比如 $IoU \ge 0.5$ 的边界框都抛弃掉；
4. 对所有边界框都进行处理，按照上述 3 个步骤来判断，抛弃还是作为输出结果；



## 4. Anchor

### anchor boxes 介绍

上述说的检测都是限制于一个格子检测出一个对象，但如果需要一个格子可以检测多个对象，那么就需要用到 anchor box。

如下图所示，假设现在输入图片是左图中的例子，在第三行的第二个格子中是刚好同时存在人和汽车，并且中心点都落在这个格子里，但根据之前的算法，只能检测到其中一个对象。而要解决这个问题，就需要用到 anchor box 了。

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/Anchor_box.png )

**这里 anchor box 的思路是预先定义两个不同形状的 anchor box**，如上图的两个，当然实际情况里可能会采用更多的 anchor box，比如 5 个甚至更多。不过这里只需要两个即可。

接着就是重新定义标签，不再是开始的 8 维向量，而是 $2\times 8$ 的向量，**前面 8 个和 anchor box1 相关联，而后面 8 个和 anchor box2 相关联**。如上图右侧的 $y$ 所示。

在实际例子中，还有一些情况：

- 比如使用两个 anchor box，但一个格子里出现 3 个对象，这种情况算法也处理不好；
- 同个格子有两个对象，但它们的 anchor box 形状也一样，这种也是算法处理不好的情况；

另外，一般怎么选择 anchor box 呢？通常是**手工指定 anchor box 形状**，选择 5-10 个不同形状的，尽量覆盖多种不同的形状，覆盖你想要检测对象的各种形状。

另一种做法是在 YOLO 后期论文中介绍的，**k-平均算法**，用它来选择一组 anchor box，最具有代表性的一组 anchor box。



### Anchor 生成

参考文章：

- [Faster RCNN之Anchors的生成过程理解](https://zhuanlan.zhihu.com/p/102978748)



参考的代码地址：https://github.com/kevinjliang/tf-Faster-RCNN

一般来说，anchor 的表现形式有两种：

- 记录左上角和右下角的坐标
- 记录中心点和宽高

所以需要注意注意标注的 anchor 形式，以及模型需要的 anchor 形式，相互之间做个转换还是比较简单的；



这份代码里生成 anchor 的函数是 `generator_anchors` ，具体代码地址：https://github.com/kevinjliang/tf-Faster-RCNN/blob/master/Lib/generate_anchors.py

```python
def generate_anchors(base_size=16, ratios=[0.5, 1, 2],
                     scales=2 ** np.arange(3, 6)):
    """
    Generate anchor (reference) windows by enumerating aspect ratios X
    scales wrt a reference (0, 0, 15, 15) window.
    """
    base_anchor = np.array([1, 1, base_size, base_size]) - 1  # [0,0,15,15]
    ratio_anchors = _ratio_enum(base_anchor, ratios)  # shape: [3,4]，返回的是不同长宽比的anchor
    anchors = np.vstack([_scale_enum(ratio_anchors[i, :], scales)
                         for i in range(ratio_anchors.shape[0])])  # 生成九个候选框 shape: [9,4]
    return anchors
```

这个函数有 3 个参数：

1. **base_size=16**：它指定的是类似感受野的区域大小，一般经过多层卷积和池化后得到的特征图，由于尺寸一般都会是原图的 $\frac{1}{2^n}$ ，其每个点对应原图都是一个区域，这里默认是 16 的话，表示对应原图里 $16\times 16$ 的区域大小；
2. **ratios=[0.5, 1, 2]**：这个参数是对基本的区域大小进行 3 种比例的变换，即 `1:2, 1:1, 2:1`，如下图所示，其实就是生成了 3 种不同形状的 anchor boxes

<img src="https://gitee.com/lcai013/image_cdn/raw/master/notes_images/anchor%20%E7%94%9F%E6%88%90%E7%90%86%E8%A7%A31.png" style="zoom:50%;" />

3. scales=2**np.arange(3, 6)：这个作用就是对 anchor 进行指定倍数的增大，这里是以 2 为底数，然后是指数分别是 3，4，5，即分别放大了 8，16 和 32 倍，如下图所示：

<img src="https://gitee.com/lcai013/image_cdn/raw/master/notes_images/anchor%20%E7%94%9F%E6%88%90%E7%90%86%E8%A7%A32.png" style="zoom:50%;" />

首先对于第一个参数，定义一个基本的 anchor，这里代码是这样操作的：

```python
base_anchor = np.array([1, 1, base_size, base_size]) - 1 
```

直接定义了一个左上角为 （0，0），右下角是（15，15）的 $16\times 16$ 的 anchor。

接下来是实现生成多个不同宽高比的 anchors，实现代码如下：

```python
def _ratio_enum(anchor, ratios):  # 这个函数计算不同长宽尺度下的anchor的坐标
    """
    Enumerate a set of anchors for each aspect ratio wrt an anchor.
    """
    w, h, x_ctr, y_ctr = _whctrs(anchor)  # 找到anchor的中心点和长宽
    size = w * h  # 返回anchor的面积
    size_ratios = size / ratios  # 为了计算anchor的长宽尺度设置的数组：array([512.,256.,128.])
    # 为什么要开根号来获得ws呢？我这里理解成通过面积来计算正方形的边长作为ws
    # 使ws:hs满足对应比例的同时，并且ws*hs的大小与base_anchor的大小相近的一种策略
    ws = np.round(np.sqrt(size_ratios))  # 计算不同长宽比下的anchor的宽：array([23.,16.,11.])
    hs = np.round(ws * ratios)  # 计算不同长宽比下的anchor的长 array([12.,16.,22.])
    # 请大家注意，对应位置上ws和hs相乘，面积都为256左右
    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)  # 返回新的不同长宽比的anchor 返回的数组shape:[3,4]，请注意anchor记录的是左上角和右下角的坐标
    return anchors
```

 这里生成不同宽高比的 anchor 方法，是通过这样的步骤：

1. 计算基础 anchor 的面积；
2. 根据给定的 ratios，计算不同宽高比下的面积
3. 对面积开方得到宽，然后乘以 ratio，得到长度

通过这种方式是即得到要求的宽高比，同时新的 anchor 的面积也和基础 anchor 的面积比较接近，比如上述得到新的anchor 的宽高分别是 `[23, 12], [16,16], [11,22]` ，其面积分别是 276，256 和 242，其中第二个就是基础 anchor 了；



这段代码中 `_whctrs` 是将 anchor 的表现形式转变为中心点+宽高的形式，实现如下：

```python
def _whctrs(anchor): 
    """
    Return width, height, x center, and y center for an anchor (window).
    """
    w = anchor[2] - anchor[0] + 1
    h = anchor[3] - anchor[1] + 1
    x_ctr = anchor[0] + 0.5 * (w - 1)
    y_ctr = anchor[1] + 0.5 * (h - 1)
    return w, h, x_ctr, y_ctr
```

然后生成新的 anchor 的坐标的函数如下：

```python
def _mkanchors(ws, hs, x_ctr, y_ctr):
    """
    Given a vector of widths (ws) and heights (hs) around a center
    (x_ctr, y_ctr), output a set of anchors (windows).
    给定一个宽和高的向量，以及中心点坐标，输出一组 anchors
    """

    ws = ws[:, np.newaxis]
    hs = hs[:, np.newaxis]
    anchors = np.hstack((x_ctr - 0.5 * (ws - 1),
                         y_ctr - 0.5 * (hs - 1),
                         x_ctr + 0.5 * (ws - 1),
                         y_ctr + 0.5 * (hs - 1)))
    return anchors
```

所以这里将基础 anchor 进行ratio 的变换后得到新的 3 个 anchor 就是：

```python
ratio_anchors = _ratio_enum(base_anchor, ratios)
'''[[ -3.5,   2. ,  18.5,  13. ],
    [  0. ,   0. ,  15. ,  15. ],
    [  2.5,  -3. ,  12.5,  18. ]]'''
```

接着最后一种变换就是进行 scale 的变换，即增大宽高，主要实现函数是 `_scale_enum` ：

```python
def _scale_enum(anchor, scales):  
    """
    Enumerate a set of anchors for each scale wrt an anchor.
    对每一种长宽比的 anchor，计算不同面积尺度的 anchor 坐标
    """
    w, h, x_ctr, y_ctr = _whctrs(anchor)  
    ws = w * scales  # shape [3,] 
    hs = h * scales  # shape [3,] 
    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)  # 得到不同面积尺度的anchor信息，对应的是左上角和右下角的坐标
    return anchors
```

这里对之前得到的 3 种不同宽高比的 anchor，分别进行 3 种 scale 的变换，所以最终得到的是 9 种 anchor boxes。

上述代码的实现过程也比较简单，首先将基础 anchor 变换成中心点+宽高的形式，然后根据给定的 scales 分别计算得到新的宽和高，接着就是生成对应的 anchor 的坐标。

经过这个变换得到的 9 种 anchor 坐标结果，这里是先按照 ratio 变换后的 anchor 的行维度进行遍历，因为 ratio_anchor 是 [3,4]，所以会遍历 3 次，每次输入一种宽高比的 anchor 的坐标到 `_scale_enum` 函数中，在这个函数中刚刚也说了会计算 3 种 scale 的变换，得到的是 3 种不同宽高大小的 anchor，最终结果就是输出维度是 [9,4] 的 anchor boxes 了。

```python
anchors = np.vstack([_scale_enum(ratio_anchors[i, :], scales)
                         for i in range(ratio_anchors.shape[0])])  # 生成九个候选框 shape: [9,4]
'''
[[ -84.  -40.   99.   55.]
 [-176.  -88.  191.  103.]
 [-360. -184.  375.  199.]
 [ -56.  -56.   71.   71.]
 [-120. -120.  135.  135.]
 [-248. -248.  263.  263.]
 [ -36.  -80.   51.   95.]
 [ -80. -168.   95.  183.]
 [-168. -344.  183.  359.]]
'''
```

这里文章作者也做了一个表格，更清楚不同坐标的 ratio 和 scale ：

<img src="https://gitee.com/lcai013/image_cdn/raw/master/notes_images/anchor%20%E7%94%9F%E6%88%90%E7%90%86%E8%A7%A33.png" style="zoom:50%;" />

另外这里得到的 anchor 坐标也说相对于原始图像大小的，而不是特征图上的大小，如下图所示：

<img src="https://gitee.com/lcai013/image_cdn/raw/master/notes_images/anchor%20%E7%94%9F%E6%88%90%E7%90%86%E8%A7%A34.png" style="zoom:50%;" />

根据上图，其实可以知道设计的这几种尺寸是基本可以包含图像中的任何物体了，当然除非这个物体非常大，那可能就需要调整 scale，使用更大的 scale 来包含特大的物体。







------

## 5. 候选区域

目标检测里另一个比较出名的算法，R-CNN，跟 YOLO 相比，是另一种思路，所以也基于此算法产生了很多检测算法，比如对其持续改进优化的，Fast-RCNN，Faster-RCNN 等。

R-CNN 算法是尝试找到一些区域，在这部分区域里运行 CNN 进行检测。而选择这些候选区域的方法是运行图像分割算法，分割的结果如下图所示。根据分割算法得到的结果，在不同的区域运行分类器，判断该区域是否有目标对象，比如图中标注号码的几个区域。

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/%E5%80%99%E9%80%89%E5%8C%BA%E5%9F%9F.png)


这种做法相比滑动窗口，可能是不需要图片的每个地方都去检测一遍，只需要对分割算法得到的色块，加上边界框，然后对边界框内的区域运行分类器，虽然是工作量会减少一些，但实际上速度还是很慢，但优点是精度会很不错，这个就和 YOLO 算法刚好相反。

因为速度太慢的问题，后续也有很多改进的算法

- Fast-RCNN：同样的方法进行候选区域的筛选，但通过卷积实现滑动窗口，对候选区域进行分类；
- Faster-RCNN：采用 CNN 来生成候选区域



## 6. ROI Pooling 和 ROI Align

如果你对目标检测网络 Faster R-CNN 和实例分割网络 Mask R-CNN 网络比较熟悉的话，那你应该也对这个话题非常熟悉。

在区域建议网络 RPN 得到候选框 ROI 之后，需要提取该 ROI 中的固定数目的特征（例如Faster R-CNN中的 7*7 ）输入到后面的分类网络以及边界回归网络的全连接层中。Faster R-CNN中使用的方法是 ROI Pooling，而对于像素位置精细度要求更高的 Mask R-CNN 对 ROI Pooling 进行改进，变成 ROI Align。

**ROI Pooling和ROI Align最大的区别是：前者使用了两次量化操作，而后者并没有采用量化操作，使用了双线性插值算法，具体的解释如下所示。**



### ROI Pooling 技术细节

<img src="https://gitee.com/lcai013/image_cdn/raw/master/notes_images/roi_pooling.png" alt="img" style="zoom:200%;" />

如上图所示，为了得到固定大小（7X7）的feature map，我们需要做两次量化操作：

1. 图像坐标 — feature map坐标
2. feature map坐标 — ROI feature坐标。

我们来说一下具体的细节，如图我们输入的是一张800x800的图像，在图像中有两个目标（猫和狗），狗的BB大小为665x665，经过VGG16网络后，我们可以获得对应的feature map，如果我们对卷积层进行Padding操作，我们的图片经过卷积层后保持原来的大小，但是由于池化层的存在，我们最终获得feature map 会比原图缩小一定的比例，这和Pooling层的个数和大小有关。

在该VGG16中，我们使用了5个池化操作，每个池化操作都是2Pooling，因此我们最终获得feature map的大小为800/32 x 800/32 = 25x25（是整数），但是将狗的BB对应到feature map上面，我们得到的结果是665/32 x 665/32 = 20.78 x 20.78，结果是浮点数，含有小数，但是我们的像素值可没有小数，那么作者就对其进行了量化操作（即取整操作），即其结果变为20 x 20，在这里引入了第一次的量化误差；

然而我们的feature map中有不同大小的ROI，但是我们后面的网络却要求我们有固定的输入，因此，我们需要将不同大小的ROI转化为固定的ROI feature，在这里使用的是7x7的ROI feature，那么我们需要将20 x 20的ROI映射成7 x 7的ROI feature，其结果是 20 /7 x 20/7 = 2.86 x 2.86，同样是浮点数，含有小数点，我们采取同样的操作对其进行取整吧，在这里引入了第二次量化误差。

其实，**这里引入的误差会导致图像中的像素和特征中的像素的偏差**，即将feature空间的ROI对应到原图上面会出现很大的偏差。原因如下：

比如用我们第二次引入的误差来分析，本来是2.86，我们将其量化为2，这期间引入了0.86的误差，看起来是一个很小的误差呀，但是你要记得这是在feature空间，我们的feature空间和图像空间是有比例关系的，在这里是1:32，那么对应到原图上面的差距就是0.86 x 32 = 27.52。这个差距不小吧，这还是仅仅考虑了第二次的量化误差。**这会大大影响整个检测算法的性能**，因此是一个严重的问题。



### ROI Align 技术细节

![img](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/roi_align.png)

如上图所示，为了得到为了得到固定大小（7X7）的feature map，**ROI Align技术并没有使用量化操作，即我们不想引入量化误差**，比如665 / 32 = 20.78，我们就用20.78，不用什么20来替代它，比如20.78 / 7 = 2.97，我们就用2.97，而不用2来代替它。这就是ROIAlign的初衷。

那么我们如何处理这些浮点数呢，我们的解决思路是使用“**双线性插值**”算法。双线性插值是一种比较好的图像缩放算法，它充分的利用了原图中虚拟点（比如20.56这个浮点数，像素位置都是整数值，没有浮点值）四周的四个真实存在的像素值来共同决定目标图中的一个像素值，即可以将 20.56 这个虚拟的位置点对应的像素值估计出来。

如下图所示，蓝色的虚线框表示卷积后获得的feature map，黑色实线框表示ROI feature，最后需要输出的大小是2x2，那么我们就利用双线性插值来估计这些蓝点（虚拟坐标点，又称双线性插值的网格点）处所对应的像素值，最后得到相应的输出。这些蓝点是2x2Cell中的随机采样的普通点，作者指出，**这些采样点的个数和位置不会对性能产生很大的影响**，你也可以用其它的方法获得。然后在每一个橘红色的区域里面进行max pooling或者average pooling操作，获得最终2x2的输出结果。

我们的整个过程中没有用到量化操作，没有引入误差，即原图中的像素和feature map中的像素是完全对齐的，没有偏差，**这不仅会提高检测的精度，同时也会有利于实例分割**。

![img](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/roi_align2.png)









------

# 参考

1. deeplearning.ai 04 课程第三周--目标检测
2. [Mask R-CNN详解](https://blog.csdn.net/WZZ18191171661/article/details/79453780)
3. [Faster RCNN之Anchors的生成过程理解](https://zhuanlan.zhihu.com/p/102978748)







